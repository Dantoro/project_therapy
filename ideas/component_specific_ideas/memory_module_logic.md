## Component Overview

This document summarizes the architecture and logic for the **Memory Module** in Project Therapy, including the structure and operation of Short-Term Memory (STM) and Long-Term Memory (LTM), as well as how conversational context is passed downstream for response generation.

---

### 1. Short-Term Memory (STM): Rolling Conversational Context

* **Purpose:**Store the most recent conversational exchanges at the highest level of detail, enabling immediate, context-rich analysis of user–bot interaction.
* **Structure:**

  * STM is a buffer holding **up to 10 (user input, bot response) pairs**.
  * Each pair consists of:
    - **User Input:** The cleaned user text (from BiGRU preprocessor)
    - **Bot Response:** The corresponding chatbot output
* **Operation:**

  1. **Pairing:**Each time the user provides a new input and receives a bot response, the `(user input, bot response)` pair is appended to STM.
  2. **Buffering:**STM is filled sequentially until it contains 10 pairs. It is *not* a true rolling buffer; it is periodically flushed (see below).
  3. **Flushing and Summarization:**
     When STM reaches 10 pairs, the entire buffer is passed to a **seq2seq transformer** which generates a summary block (see LTM). After summarization, STM is **erased** and begins collecting the next set of pairs.

---

### 2. Long-Term Memory (LTM): Hierarchical Summarization

* **Purpose:**Store high-level, compressed representations of earlier conversation history, allowing the system to maintain context over long sessions without retaining every individual exchange.
* **Structure:**

  * LTM is a **rotating buffer of 10 blocks**.
  * Each block contains:
    - **Text Summary:** A 5–10 sentence paragraph summarizing the corresponding 10 (user, bot) pairs in STM, generated by the seq2seq transformer.
    - **Vector Embedding:** A dense vector representing the semantic content of the summary (from the transformer or pooling layer).
* **Operation:**

  1. When STM is summarized, the resulting block is appended to LTM.
  2. If LTM already contains 10 blocks, the oldest block is dropped to make room (FIFO logic).

---

### 3. Hierarchical Attention Network (HAN): STM Context Embedding

* **Purpose:**At each user input, provide a deep, context-aware embedding of the ongoing conversation window for downstream decision-making and response generation.
* **Operation:**

  1. **Input:**The HAN runs on all sentences in STM (up to 20: 10 user inputs, 10 bot responses, in order), **plus the current user input** (making up to 21 sentences when STM is full).
  2. **Processing:**
     Each sentence is tokenized at the word level and represented via concatenated GloVe, word2vec, and learned embeddings.
     A deep (10–20 layer) HAN (BiLSTM + attention + highway/dense layers) computes an embedding representing the immediate conversational context.

---

### 4. Full Memory Cycle (at each user turn)

1. **User Input Arrives**
   - HAN is run over `[STM sentences + new user input]` to produce the STM context embedding.
2. **DecisionMaker + Chatbot**
   - Receive context from LTM (all summary blocks + embeddings), STM (cleaned text, HAN embedding), and TrendTracker.
   - Generate the bot response.
3. **Memory Maintenance:**
   - If LTM is full, drop the oldest block.
   - If STM is full (10 pairs), summarize via seq2seq and append to LTM, then clear STM.
   - Add the new `(user input, bot response)` pair as the first entry in the new or continuing STM.

---

### 5. Summary Table

| Module | Data Held            | Model Used          | Role                                        |
| ------ | -------------------- | ------------------- | ------------------------------------------- |
| STM    | 10 (user, bot) pairs | HAN (deep)          | Recent conversational context (raw + embed) |
| LTM    | 10 summary blocks    | Seq2seq Transformer | Long-term session history (summary + embed) |

---

#### Next Steps

These outputs serve as foundational features for:

* Adaptive, context-rich decision-making and chatbot generation
* Retaining both granular and abstracted memory over long conversations
* Efficient scaling to multi-session and multi-turn contexts

---

*Last updated: June 2025*
